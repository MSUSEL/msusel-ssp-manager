Minikube install
curl -LO https://github.com/kubernetes/minikube/releases/latest/download/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube && rm minikube-linux-amd64

minikube version

minikube start


Install kompose:
curl -L https://github.com/kubernetes/kompose/releases/latest/download/kompose-linux-amd64 -o kompose
sudo mv kompose /usr/local/bin/kompose
sudo chmod +x /usr/local/bin/kompose
kompose version


Generate the manifest files for the app using kompose:
cd path/to/app/docker-compose.yml
kompose convert -f docker-compose.yml

Now we want to deploy those files to the cluster. (Do this in the directory where the manifest are located)
kubuctl apply -f filename

We need to build the images in the Minikube docker engine (Minikube has a docker engine independent from the host's docker engine)
First enable the minikube docker engine registry so that it can store images:
minikube addons enable registry

ernesto@ernesto-VirtualBox:~/Documents/Ugov_Services/user-registration$ docker build -t localhost:5000/ugov-services:latest .
ernesto@ernesto-VirtualBox:~/Documents/Ugov_Services/user-registration$ docker tag localhost:5000/ugov-services:latest ugov-services:latest
ernesto@ernesto-VirtualBox:~/Documents/Ugov_Services/user-registration$ minikube image load ugov-services:latest
ernesto@ernesto-VirtualBox:~/Documents/kubernetes-manifests/ugov-services$ minikube ssh

docker@minikube:~$ docker images | grep ugov-services


sudo snap install kubectl --classic

We'll build our images and load them to minikube:
docker compose build
minikube image load msusel-ssp-manager-flask:latest
minikube image load msusel-ssp-manager-react-app:latest

If you make changes and rebuild the images, you have to load them again in minikube. This process takes a while:
minikube delete 
minikube start
minikube addons enable registry
minikube image load msusel-ssp-manager-flask:latest
minikube image load msusel-ssp-manager-react-app:latest
minikube image load oscalprocessing:latest
minikube image load arangodb:3.8.1
minikube image load bronbootstrap:latest (WWWe haaaaaaven't cccreated manifeest for this service)
minikube image load msusel-ssp-manager-driver:latest

minikube ssh
# Create directories for ArangoDB
sudo mkdir -p /var/lib/arangodb3
sudo mkdir -p /var/lib/arangodb3-apps
sudo mkdir -p /data  # This is for the bootstrap container

# Set permissions
# ArangoDB typically runs as arangodb user (uid 999)
sudo chown -R 999:999 /var/lib/arangodb3
sudo chown -R 999:999 /var/lib/arangodb3-apps
sudo chmod -R 700 /var/lib/arangodb3
sudo chmod -R 700 /var/lib/arangodb3-apps

# For bootstrap data
sudo chmod -R 777 /data

exit

# Create namespace (optional but recommended)
kubectl create namespace bron
The `kubectl create namespace bron` command creates a new namespace in your Kubernetes cluster called "bron".
A namespace in Kubernetes is a way to create a logical isolation between resources in your cluster. Think of it like a virtual cluster within your physical cluster. Namespaces help you:
1. Organize resources into groups (e.g., keeping all BRON-related resources together)
2. Avoid naming conflicts between different projects
3. Control resource access and quotas for different teams or projects
4. Keep your cluster resources organized and separated
Without specifying a namespace, all resources are created in the `default` namespace. By creating a dedicated `bron` namespace, we:
- Keep all BRON-related resources (deployments, services, secrets, etc.) organized together
- Make it easier to manage and clean up (you can delete all BRON resources by deleting the namespace)
- Prevent conflicts with other applications in your cluster
- Make it clearer which resources belong to the BRON system
After creating the namespace, you would need to add `-n bron` to your kubectl commands or add `namespace: bron` to your manifest metadata to deploy resources into this namespace. Would you like me to show you how to modify the manifests to use this namespace?

# Apply all manifests (they will automatically go to the bron namespace)
# Apply all storage and secrets first
kubectl apply -f arango-root-password-secret.yaml
kubectl apply -f brondb-data-container-persistentvolumeclaim.yaml
kubectl apply -f brondb-apps-data-container-persistentvolumeclaim.yaml
kubectl apply -f bootstrap-data-container-persistentvolumeclaim.yaml

# Deploy ArangoDB
kubectl apply -f brondb-deployment.yaml
kubectl apply -f brondb-service.yaml

# Wait for ArangoDB to be ready
kubectl wait --for=condition=available deployment/brondb --timeout=300s

# Deploy bootstrap job
kubectl apply -f bootstrap-deployment.yaml

# Wait for bootstrap to complete
# Monitor the bootstrap job progress
kubectl logs -f -n bron job/bootstrap

# In a separate terminal, you can check the status
kubectl get jobs -n bron

# Wait for completion with extended timeout (1 hour)
kubectl wait --for=condition=complete job/bootstrap -n bron --timeout=3600s

# Deploy driver job
kubectl apply -f driver-deployment.yaml

# To check resources in the bron namespace
kubectl get all -n bron

# To check specific resource types
kubectl get pods -n bron
kubectl get services -n bron
kubectl get jobs -n bron


#In case of trouble
kubectl delete deployment -n bron brondb (for example)


kubectl apply -f .

Check that everythingis running:
ernesto@ernesto-VirtualBox:~/Documents/msusel-ssp-manager$ kubectl get pods
NAME                        READY   STATUS    RESTARTS   AGE
flask-65f95ddb99-8rvmx      1/1     Running   0          102s
opa-555fdfc45d-x6rzd        1/1     Running   0          102s
react-app-9bd979778-mlnkg   1/1     Running   0          102s




ernesto@ernesto-VirtualBox:~/Documents/msusel-ssp-manager$ minikube ip
192.168.49.2

ernesto@ernesto-VirtualBox:~/Documents/msusel-ssp-manager$ kubectl get services
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
app          NodePort    10.108.222.38    <none>        8000:31621/TCP   5d14h
db           ClusterIP   10.100.191.169   <none>        5432/TCP         5d14h
flask        ClusterIP   10.96.197.70     <none>        5000/TCP         5d14h
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP          5d16h
opa          ClusterIP   10.96.109.202    <none>        8181/TCP         5d14h
react-app    NodePort    10.110.248.48    <none>        3000:32000/TCP   5d14h


==> react-app is found at 192.168.49.2:32000



minikube image load oscalprocessing:latest

We need to create the volumes inside minikube:
minikube ssh

# Create directories
minikube ssh
sudo mkdir -p /flask/shared
sudo mkdir -p /flask/generatedFiles
sudo chmod -R 777 /flask
sudo chown -R 1000:1001 /flask

# Ensure docker.sock is accessible
sudo chmod 777 /var/run/docker.sock

exit



To delete volumes in minikube:
ernesto@ernesto-VirtualBox:~/Documents/msusel-ssp-manager$ for pvc in flask-claim1 flask-claim2 flask-claim3 flask-claim4 flask-claim5 flask-claim6 flask-claim7; do
  kubectl patch pvc $pvc -n default -p '{"metadata":{"finalizers":null}}' --type=merge
done
persistentvolumeclaim/flask-claim1 patched
persistentvolumeclaim/flask-claim2 patched
persistentvolumeclaim/flask-claim3 patched
persistentvolumeclaim/flask-claim4 patched
persistentvolumeclaim/flask-claim5 patched
persistentvolumeclaim/flask-claim6 patched
persistentvolumeclaim/flask-claim7 patched
ernesto@ernesto-VirtualBox:~/Documents/msusel-ssp-manager$ kubectl get pvc -n default -o json | jq '.items[] | {name: .metadata.name, finalizers: .metadata.finalizers}'
ernesto@ernesto-VirtualBox:~/Documents/msusel-ssp-manager$ kubectl delete pvc flask-claim1 flask-claim2 flask-claim3 flask-claim4 flask-claim5 flask-claim6 flask-claim7 -n default --wait=false
Error from server (NotFound): persistentvolumeclaims "flask-claim1" not found
Error from server (NotFound): persistentvolumeclaims "flask-claim2" not found
Error from server (NotFound): persistentvolumeclaims "flask-claim3" not found
Error from server (NotFound): persistentvolumeclaims "flask-claim4" not found
Error from server (NotFound): persistentvolumeclaims "flask-claim5" not found
Error from server (NotFound): persistentvolumeclaims "flask-claim6" not found
Error from server (NotFound): persistentvolumeclaims "flask-claim7" not found
ernesto@ernesto-VirtualBox:~/Documents/msusel-ssp-manager$ kubectl get pv -o json | jq '.items[] | {name: .metadata.name, finalizers: .metadata.finalizers}'
ernesto@ernesto-VirtualBox:~/Documents/msusel-ssp-manager$ kubectl delete pv --all --wait=false
No resources found
ernesto@ernesto-VirtualBox:~/Documents/msusel-ssp-manager$ kubectl get pvc -n default
No resources found in default namespace.
ernesto@ernesto-VirtualBox:~/Documents/msusel-ssp-manager$ kubectl get pv
No resources found





Debugging:
kubectl logs flask-6f9779c4c7-9qgxg

You can execute commands inside the container running on the pod:
ernesto@ernesto-VirtualBox:~/Documents/msusel-ssp-manager$ kubectl exec -it flask-74cb577799-t2rlw -- pwd
/workdir
ernesto@ernesto-VirtualBox:~/Documents/msusel-ssp-manager$ kubectl exec -it flask-74cb577799-t2rlw -- ls
Dockerfile     OSCAL_Test_Files  app	 artifactsExamples  generatedFiles  output_vulnerability_effectiveness	requirements.txt  shared
Documentation  __pycache__	 app.py  flaskTree.txt	    oscal_schemas   react-app				run.py		  tests
ernesto@ernesto-VirtualBox:~/Documents/msusel-ssp-manager$ kubectl exec -it flask-74cb577799-t2rlw -- ls /shared
profile.yaml  validation.txt






VALIDATE ROUTE: file uploaded failed
The problem here is that oscalprocessing is not finding /shared/profile.yaml but this is particular to minikube. Might be better to just solve it for Azure.

YOU HAVE TO REBUILD AND LOAD THE IMAGES AFTER ANY CHANGES
